**Draft warning:** this is a public draft dated 12 Aug 2015, but should not be considered final. 

# Introduction

The internet was created in an era when people used fewer computers and computers had fewer users. In this primordially-networked soup, there was a strong and easily-understood relationship between physical network addresses and the independent agents they represented. As the internet evolved into the ever-growing web, these early assumptions have grown increasingly invalid, and the haphazard collection of incremental bandages we've created to reconcile the network topology with the people and things who use it has grown increasingly unwieldy. This unavoidable negotiation between the *network*-oriented data *transport* and the *agent*-oriented data *use* is one of the most fundamental problems facing the internet today, and there is a clear need to address it directly. The Muse combines new and old ideas to construct exactly such a mediator.

This core problem has profound and economically-compelling effects on the internet as a whole. Crucially,

1. Internet-connected devices and services are especially difficult to develop,
2. Privacy is complicated to provide and harder to demand, and 
3. It's very difficult to locate things and people.

Meanwhile, time and experience have suggested that any proposed solution must fulfill two constraints:

1. Information must be universal, and
2. Information must be self-contained.

Put simply: because it lacks any inherent definition of individual agency, information on the web is hard to protect, hard to find, and hard to use. Solving this requires a single system applicable to anyone, and such a thing should (and must!) be financially beneficial to the internet community as a whole. The Muse proposal entails:

1. Agent-to-agent (device-independent end-to-end) encryption, enabling
2. A singular, internet-wide private-by-default data persistence system, with
3. System-wide, granular management of information sharing, which requires
4. A minimal but universal agent identity mechanism.

The Muse is a global, private, secure information management system that can be used to connect any network node with an inherent awareness of the persons, services, or things interacting with online data. It eliminates the need for site-specific accounts, and allows application developers to focus directly on creating network value for people and things, regardless of their physical network location, and with maximum cross-application compatibility.

# Internet [agency](http://en.wikipedia.org/wiki/Agency_%28philosophy%29), and the lack thereof

Broadly speaking, one can do exactly four things with information: create it, retain it, share it, and access it. Information agency is the degree to which "agents" (persons, services, things) can exert autonomous, independent control of those four functions. 

Because information cannot spontaneously coalesce, its creation is a process that is explicitly impossible for any non-agent. The role of any network, then, is to facilitate data sharing, access, and potentially retention. Democratization of information *access* has long been a hallmark of the internet, but in the age of Facebook and Twitter, information sharing and retention are increasingly centralized along mutually-incompatible platform lines. Though these platforms have enabled previously unimaginable network proliferation and dramatically increased the accessibility of data, their commodification of digital community have led to extreme platform node-locking and low user satisfaction. The historical absence of any unified sharing and retention protocol has substantially contributed to this problem.

The internet -- the *inter-network*, a "network of networks" -- was developed in the 70's and 80's to unify separate computer groups with a common data exchange system. It allowed any two online computers to talk to each other in a consistent way. By the early 90's, the internet needed a way to manage and share documents between these connected machines, and so the "world wide web" was born. In these days, most content creators had direct or indirect control over physical data accessibility, and could quite easily add or remove information from the network. Most of the core architecture of the web (DNS, domains, URLs, websites) was established in those first few years, and other than incremental improvements meant to sustain existing functionality, it hasn't changed much since.

This was an effective and reasonable approach to physical network management, but it did nothing to address the relationship between data producers/consumers and their network locations. At the time, there simply wasn't a need: with so few machines, and so few people using them, there was a much stronger correlation between network location and agent identity. But without an intrinsic definition of the connection between data and author, agency was impossible to protect. As the web quickly outgrew its cradle, that relationship uncoupled from network functions, and new internet applications were forced to adapt core web architecture to demands it was never intended to serve.

With [20 billion devices online already, and another 20 billion projected in the next five years](http://www.forbes.com/sites/gilpress/2014/08/22/internet-of-things-by-the-numbers-market-estimates-and-forecasts/), we have reached the tipping point. The fracturing of the web between a small number of discrete mega-platforms is tremendously deleterious to the internet's potential, and the longer we wait, the bigger these problems become. The band-aids just aren't stopping the bleeding anymore, and it's time to directly address the question of personal agency in the digital age.

## Application development without inherent agency definition is exceptionally complicated

An enormous amount of effort is spent on the haphazard, redundant systems we've created to address the agent-less internet. Arguably, the entire multibillion-dollar enterprise identity and access management (IAM) market has arisen as a result. But far more profound are the results on consumer web service development: the vast majority require site-specific accounts, and each and every one of these companies must devote significant resources to account services. Single sign-on systems, in response, have emerged to provide authentication services, but still rely primarily upon individual sites to perform account management. Throughout this entire exchange, both consumer and developer experience remains indescribably fragmented.

It's difficult to quantify the economic effects of this systemic lack of agency, since there aren't really any existing analogues. However, information security (infosec) and IAM market estimations are a reasonable starting point. In 2015, the global infosec market is [projected to top $75 billion](http://cybersecurityventures.com/cybersecurity-market-report/). Meanwhile, the enterprise IAM market is expected to grow to [roughly $7 billion by 2017](http://venturebeat.com/2014/09/26/the-cloud-is-disrupting-and-defragmenting-the-identity-market/). Combined with [incredibly bullish](http://www.forbes.com/sites/gilpress/2014/08/22/internet-of-things-by-the-numbers-market-estimates-and-forecasts/) predictions of growth in the Internet of Things market, where a proper IAM solution is yet to be presented and [privacy is an enormous consumer concern](http://fortune.com/2015/07/06/consumer-data-privacy/), these numbers suggest that "agency as a service" could see a market size in the $10-20 billion (or greater) range within the next 5-10 years. Given the effects a private-by-default platform would have on consumer targeting, it's safe to say that it would also stimulate a fair amount of upheaval in the [$121 billion](http://techcrunch.com/2014/04/07/internet-ad-spend-to-reach-121b-in-2014-23-of-537b-total-ad-spend-ad-tech-gives-display-a-boost-over-search/) internet advertising market.

These numbers make it clear that the persistence of the status quo is certainly not a product of market apathy. There is a clear consumer demand for privacy, and a clear need for integrated consumer experience. In a network environment with no concept of agency, developing applications to meet these market demands is a tremendously challenging and expensive prospect. If the network is instead made agent-aware, such concerns are far simpler to address: information retention can be handled independently from information access, and agents can make simple, unencumbered decisions about information sharing, substantially reducing development complexity. There is therefore a strong case for an agency protocol on an ever-growing internet.

## Agency requires privacy

Privacy is a measure of information dissemination. A completely private record is known to exactly one agent: its author. Though networks derive their utility through communication, this should not come at the expense of agency; the agent should have explicit control over what information the network retains or shares on her behalf. To reconcile these competing interests, information **must** be treated as *private by default*. In the same way that our thoughts are private until we speak them, our uploads must be private until we share them. Any other arrangement precludes individual agency; automatic information sharing cannot be a condition of use.

It's worth noting that security is a necessary but insufficient condition for privacy. Security, in a traditional network sense, concerns itself with the protection of data between network locations. Privacy, on the other hand, questions the protection of data between network agents. Email over https, for example, protects both alice@example.com and bob@example.com from network eavesdropping, but offers limited protections of privacy: the example.com mailserver is still free to inspect their email. On the web in general, nonexistent protocol-level privacy protection is a core barrier to agency, and since its 20-year-old foundations require neither security nor privacy, much internet traffic is left public.

With more than 3 billion people on the internet today, universally public storage of information (for example, a bank account) is obviously a very bad idea, so plenty of secure sites do exist. But the current approach affords security only on a per-site basis, making it incompatible with any open, cross-site data sharing standards. And even if https were required for all websites, it still offers no privacy protection: agents are left to choose between trusting megasites like Google with *all* their information, or wholly avoiding the services the sites provide. This paradigm is as relevant to Facebook's advertising practices as it is a hospital's electronic records system. Without agency, the internet is a mess, and without privacy, there can be no real agency.

## URLs decrease accessibility; privacy renders them irrelevant

URLs were meant as a convenient, comprehensible data locator for both human and computer. They were standardized in [December 1994](https://en.wikipedia.org/wiki/Uniform_resource_locator#History), at a time when the total number of websites had [just passed 10000](https://en.wikipedia.org/wiki/List_of_websites_founded_before_1995#1994). Since then, the web has grown by [several million times](http://www.worldwidewebsize.com/), but the ```http://www.domain.com/file/path/here``` scheme has remained essentially unchanged. 

However, in the face of exponential information growth, many (if not most) popular web services eschew URL legibility entirely. Meanwhile, 404 errors are so common that the term has [entered conversational English](http://www.webster-dictionary.org/definition/404). URLs have clearly not aged well: their quest for singular reconciliation between machine and human accessibility has made them a hindrance to both. The practical reality is that search engines ([including social networking](http://qz.com/333313/milliions-of-facebook-users-have-no-idea-theyre-using-the-internet/) sites with search functions) are now the overwhelmingly predominant means of human data location. URLs, which are used increasingly only by computers, remain a particularly unreliable means of machine data location. Furthermore, since developers must navigate both machine and computer addressing, the complicated tangle of links between unintelligible URLs greatly compounds development difficulty. This has a profound effect, even (and perhaps especially) when those URLs are used as the one [pseudo-standardized](https://en.wikipedia.org/wiki/Representational_state_transfer) way to interface web services.

One of the few remaining primary roles of URLs (and in particular domains) is to assure network security through DNS and SSL/TLS. But a robust global agent-to-agent privacy protocol is a far stronger protection: information will be safely transferred between agents *regardless of the server hosting the communication*. This renders URLs' security functions irrelevant, allowing search engines to focus exactly and exclusively on mediating between arbitrary machine addresses and conceptual human ones.

## The universality constraint

The final goal of the project is to bring agent-based coherency to the internet. This must be a universally-applicable effort; the Muse must be capable of managing any digital information. This cuts to the heart of [standards proliferation](https://en.wikipedia.org/wiki/Format_war): it's functionally impossible to be *everything* for *everyone*. Proper division of concerns and choice of abstractions are absolutely crucial.

Instead of being everything for everyone, we focus on being *something* for *anyone*. The Muse [won't force people to behave](http://www.paulgraham.com/design.html); it attempts to establish a bare minimum standard for agency in any networked data communication -- no more, no less. That's intended to be just as true for developers as it is for consumers, so it must be designed to support exactly that abstraction with minimum technical overhead, for any arbitrary network bytestream.

## The self-containment constraint

Metadata is prolific on the contemporary web, but when stored externally to the information, it is very easily lost. The original author of a Youtube video is stored externally, on Youtube's servers, and is immediately lost when the video is re-uploaded to Facebook. In contrast, image metadata, though very easily removed, is notoriously "sticky", to the point where it's been used as evidence of Russian involvement in the current Ukrainian conflict. For metadata to be universal, it must be internalized.

Metadata can also be incredibly revealing. Public metadata, like geolocation information on a Twitter feed, can be used to reconstruct a very detailed picture of someone's life. And from an information security standpoint, public metadata *by definition* [leaks confidential information](http://gizmodo.com/why-the-metadata-the-nsa-has-on-you-matters-512103968). So since privacy is a requirement for agency, information must be self-contained.

Furthermore, to allow for independent, modular architectural innovation, the Muse cannot simply assume the existence of supporting transport infrastructure. It should be capable of operation on any message-based reliable bytstream, and so it must be able to create any required network functions using basic network primitives and without accompanying metadata. Muse files must be wholly self-contained.

# The Muse project

The goal, then, of the Muse project is to serve as an open standard for data agency on any network. It exposes a singular API for inter-agent communication that is independent of the underlying transport architecture. It will, at least temporarily, require a centralized platform for infrastructure support, primarily to maintain compatibility with existing internet architecture during the adoption phase. However, the Muse minimizes switching costs between transport systems: new physical networks require only a message-based reliable bytestream and a single transport-specific adapter method.

Applications built on the Muse should expect the network to deliver an asynchronous, secure bytestream from one agent to another. Any information-producing or -consuming entity is an agent on the network; there is no global client/server distinction on the Muse. If an internet-connected thermostat wants to communicate an apartment's temperature to its tenant, the Muse constructs an asynchronous bytestream between the thermostat and the tenant, regardless of their respective network locations. Muse implementations may facilitate that connection over any or every network connection available, even over multiple transport technologies.

This is made possible by: 

1. Agent-to-agent encryption / privacy-by-default
2. Network-supported data persistence
3. An information sharing/access standard
4. An identity management mechanism

## 1. Agent-to-agent encryption

Agent-to-agent encryption is the backbone of the Muse. It is a stronger constraint than [end-to-end encryption](https://en.wikipedia.org/wiki/End-to-end_encryption), which is traditionally concerned only with information flow between specific *devices*, and not the persons or things using them. In contrast, agent-to-agent encryption is intended to support multiple identities per device, and multiple devices per identity. However, ultimate realization of this paradigm will also require changes at the device level (which are well beyond the scope of the Muse).

Agent-to-agent encryption allows the Muse to be host-agnostic, exposing a singular agent-to-agent API. This enables implementations to very cheaply switch between transport technologies, enabling, for example, a [meshed Bluetooth](http://www.forbes.com/sites/michaelwolf/2015/02/26/with-mesh-bluetooth-strengthens-case-as-key-internet-of-things-technology/) P2P provider to coexist with a traditional TCP/IP connection. Any data store or server, owned by any party, can be used for data persistence, without increased risk to data confidentiality or authenticity. Data availability is then provided by the sum total of Muse-connected data stores, and this is a significant target for future development. The prospect of distributed, algorithmically-assured availability and redundancy, in a similar vein to [Maidsafe](http://maidsafe.net/) or [Storj](http://storj.io/), is particularly enticing. However, we expect that large centralized "storage providers" will emerge to fill the interim gap between infant technology and mature network.

Muse content is symmetrically encrypted within "encrypted information container object" (eico) files. There is built-in support for future expansion into multiple cipher suites. These files contain a single piece of public metadata: a unique identifier for their author, who must cryptographically sign the object. The opened container should itself be treated as an arbitrary bytestream, though the several-hundred-byte public header makes this a potentially inefficient transport mechanism for small streams.

All EICO files are static. This warrants some discussion: there is an important distinction between computer network locations and human conceptual mappings. Take, for instance, the earlier example of a network-connected thermostat. A conceptual map of its recorded temperature will have different content for different contexts: yesterday's temperature may be different from today's. However, each of those individual data points are static, fixed in time. EICO files are the latter: snapshots that may be generated for any content, at any speed, but that (once generated) remain forever unchanged. Dynamic conceptual mappings, however, *are* supported by the Muse persistence system.

## 2. Standardized network-supported persistence

Because agents may or may not be available to the physical network at any given point in time, or may be simultaneously "present" at more than one network location, the Muse must function as a global asynchronous communication platform. Asynchrony requires buffers, and on the Muse this function is served by the network itself, which is treated as a single simple [flat](https://en.wikipedia.org/wiki/File_system#Flat_file_systems) data store. 

There are two kinds of Muse persistence: static and dynamic. When encrypted EICO containers are uploaded to the Muse, their author binds an address to it (the "Muse unique identifier", or MUID). In both static and dynamic persistence, this address is a [zero-knowledge](https://en.wikipedia.org/wiki/Zero-knowledge_proof), [content-based](https://en.wikipedia.org/wiki/Content-addressable_storage) unique identifier. Static binding addresses reuse the file hash of the EICO itself. Dynamic bindings reference these static addresses to construct an arbitrary-length first-in, first-out buffer. Each dynamic binding is deterministically assigned a static muid based on its original buffer.

A binding may be removed by its creator at any time. This debinding process signals to the network that the object may be garbage collected by storage providers. However, the creation of bindings is not limited to an EICO's author; any Muse identity may bind to an object, and EICO removal only proceeds when all bindings have been removed. Any Muse identity may therefore prevent the object's removal. However, the authenticated binding party is included in the binding as public information. This compromise is intended to strike a balance between preserving the historical data record and retaining individual data agency. A malicious author's deletion may be blocked by any party, but any malicious blocker is subject to public pressure from the author. This system seems particularly effective in light of numerous international discussions surrounding data retention.

Muse implementations wrap the transport layer with a bidirectional [publish/subscribe](https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern) adapter protocol. By necessity, these subscriptions are managed on a per-network-location basis and *do not authenticate the requestor*, however, as all EICO are public and independent from access management, this should not appreciably increase their security footprint. As such, though the underlying transport layer may depend upon a traditional client-server model, the Muse itself is wholly peer-to-peer (or, in our terms, agent-to-agent). This asynchronous network persistence mechanism lays the foundation for all higher-level Muse objects, including the identity and access management interfaces.

This standardized, network-wide asynchronous persistence layer is incredibly powerful. Because bindings have no expiration, it can be used for hosting data at rest (for example, a website) just as easily as ephemeral communications (like our earlier thermostat). The full power of asynchronous communications can be applied to the network, enabling technologies as diverse as highly intelligent DoS attack mitigation to distributed cloud computing. It makes the conceptual dream of "saving something to the cloud" a practical reality.

## 3. Information sharing and access

Encrypted containers require key infrastructure. Symmetric keys for EICO are normally distributed with a standard syntax over dynamically-bound API pipes between agents. This creates a low-level secure "conversation" between exactly two agents, where encryption keys are passed via muid/key pairs. This key is not agent-specific, and as such (and mimicking physical reality), any agent may then share that key with third parties. This is also in keeping with the philosophy of data agency: once access has been shared, the recipient agent must be allowed to exercise her own control over re-sharing that information.

Higher-level API pipes may be constructed from this low-level key sharing interface, enabling the creation of arbitrary numbers of API [endpoints](https://stackoverflow.com/questions/5034412/api-endpoint-semantics). Any API pipe may always be closed or reopened, and forward secrecy is entirely achievable through careful management of session length and dynamic binding buffer size.

Since dynamically-bound EICO pipes are themselves encrypted objects, there must first be an API handshake to overcome the "chicken and the egg" problem. This is accomplished with an asymmetrically-encrypted API requester (the "encrypted information access request", or EIAR). EIAR are rigidly-formatted, with a single piece of public metadata: the muid of the intended recipient's identity. Within the encrypted body, visible only with the recipient identity's private key, are included the muid of the API requester (the identity, not the EIAR), the muid of the desired API pipe, and its symmetric key. The entire EIAR is then signed by its requester.

Upon receipt, an API provider decrypts the request, opening a downstream dynamic API pipe and establishing bidirectional communication as a response. The recipient may then debind the EIAR, freeing it for garbage collection. Though this handshake process can be used to establish *any* API pipe, due to large differences in computation times for asymmetric and symmetric encryption, it is intended primarily to establish the low-level symmetric key exchange session.

## 4. Network identity management

Finally, to support keysharing, the Muse requires identity infrastructure. Agent identities must be capable of generating digital signatures, asymmetric encryption, and (preferably deniable) alias verification. As such, a Muse identity consists of three components: an asymmetric signature key, an asymmetric encryption key, and an asymmetric Diffie-Hellman public key. These identities are self-hosted on the Muse network in specially-formatted EICO files. The first two functions of an identity (signing and encryption) have already been discussed, but the most interesting of them remains: deniable alias exchange. 

Since Muse identities *may* be temporary (ie, named, pseudonymous, or anonymous), it's important for an identity's alias to be capable of proving its origin. Put "simply", if the ```Alice``` identity has an alias ```DefinitelyNotAlice```, it is very useful for ```DefinitelyNotAlice``` to be able to prove to ```Bob``` (or even ```DefinitelyNotBob```) that she is, in fact, equivalent to ```Alice```. This task is, in and of itself, simple: ```DefinitelyNotAlice``` can sign *almost anything* with ```Alice```'s signature key. However, this exchange would allow ```Bob``` to prove to an outside observer that ```Alice``` = ```DefinitelyNotAlice```. It would be better if that proof were meaningful to ```Bob```, but meaningless to an outside observer. On the Muse network, this is accomplished with a [Diffie-Hellman](https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange) exchange between ```DefinitelyNotAlice``` and ```Bob```, using the public keys from ```Alice``` and ```Bob```. If Bob shares the exchange, he has no way of proving he hasn't constructed it himself; the proof that ```Alice``` = ```DefinitelyNotAlice``` is wholly contingent upon trust in ```Bob```.

It's important to note that the identity infrastructure inherent to the Muse does **not** constitute verification of physical identity. Just because ```Alice``` says she's actually Alice doesn't provide any assurance that they are the same living, breathing agent. This function must be fulfilled by supporting overlay infrastructure, and a common API for this purpose will likely be an early target for network innovation. Providing only the most basic necessary *network identity* allows applications to bootstrap more complete identities from the basic network primitive, allowing webs-of-trust to seamlessly coexist with trusted third-party identity verification, key escrow services, direct exchange of identity between individuals, etc.

# Agent-oriented architecture: the conclusion

The internet shuttles information from computer to computer, but we use it to connect from person to person. At the end of the day, that's the single largest functional disconnect between the way the web works and the way it was designed. And though it is necessary and unavoidable, this network-oriented description of data's path between machines is insufficient for our technical needs. Communications are, by their nature, *agent-oriented*: they are said by one entity, to another; much of the fragmentation of the web today is an attempt to reconcile these two worlds.

The Muse presents a unified standard for communication directly between persons, things, and services, regardless of their respective network locations. This paradigm, which already forms the basis for internet accounts, becomes codified and streamlined. In so doing, users are afforded agency, and sites need not waste precious development time reinventing the wheel. The Muse is designed to make creation as easy as possible; "just put it on the internet" becomes a practical reality, not just a conceptual prescription.

Given the messy state of the internet today and the indications of massive future growth, this is an audacious but necessary change. It is, however, only possible to implement if agents have a definable identity. That identity must be shared, and the global needs for these identity and sharing mechanisms cannot possibly be predicted in advance; they must instead grow on their own terms from bare minimum network primitives. By shifting these systems from individual websites to the network that supports them, a significant development burden is likewise relocated. Such a protocol can bridge the gap between network and agent in an efficient, desirable way. This system is the Muse.